# DeepBVP: Гибридный ИИ-решатель краевых задач

**DeepBVP** — это исследовательский проект, посвященный созданию интеллектуального решателя для нелинейных краевых задач (BVP) второго порядка. Система использует глубокое обучение для генерации «умного» начального приближения, что радикально повышает вероятность сходимости и устойчивость алгоритмов в жестких задачах.



## Постановка задачи

Проект нацелен на решение ОДУ на отрезке $x \in [0, 1]$ следующего вида:

$$
\begin{cases} 
\varepsilon y'' + p(x)y' + q(x)y + j y^2 + k y^3 = f(x) \\
y(0) = \alpha, \quad y(1) = \beta 
\end{cases}
$$

### Расшифровка функций и диапазоны:
* **Параметр ε (epsilon)**: $[10^{-4}, 10^{-1}]$. Определяет «жесткость» уравнения. При малых значениях формируется погранслой — область резкого изменения решения.
* **Коэффициент переноса**: $p(x)$ = $p_0 + p_1 x + p_2 x^2 + w_1 \sin(w_2 x) + v_1 \cos(v_2 x)$
* **Коэффициент реакции**: $q(x)$ = $q_0 + q_1 x + q_2 x^2 + e_1 \sin(e_2 x) + u_1 \cos(u_2 x)$
* **Источник**: $f(x)$ = $A \exp\left(-\left(\frac{x-\mu}{\sigma}\right)^2\right) + c_0 + c_1 x + c_2 x^2$
    * $\mu \in [0.2, 0.8]$ (центр пика), $\sigma$ (ширина).
* **Нелинейность**: Коэффициенты $j$ (при $y^2$) и $k$ (при $y^3$) в диапазоне $[-2.0, 2.0]$.

---

## Датасет и процесс обучения

Для обучения модели был сгенерирован уникальный синтетический датасет объемом **27 000 задач**:

1.  **Генерация**: Параметры (25 входных признаков) выбирались методом случайного сэмплирования из физически обоснованных диапазонов.
2.  **Решение**: Каждая задача решалась численным методом `solve_bvp` с высокой плотностью сетки и жесткими допусками по точности.
3.  **Аппроксимация**: Вместо предсказания значений в точках, каждое решение аппроксимировалось набором из **16 коэффициентов кубических B-сплайнов**. Это позволило сжать информацию без потери аналитической гладкости.



---

## Почему классический ML не подошел?

В ходе исследования проводились эксперименты по замене нейросети классическими алгоритмами (Random Forest, XGBoost, CatBoost) для предсказания тех же 16 коэффициентов сплайна.

* **Результат**: Точность классического ML оказалась значительно ниже требуемой для качественного старта решателя.
* **Причина**: Древовидные модели и ансамбли не смогли эффективно выявить сложные непрерывные нелинейные зависимости между входными физическими параметрами и весами базисных функций.
* **Вывод**: Только глубокие полносвязные нейросети (DL) с функциями активации типа GELU смогли построить качественную «карту решений», обеспечив точность (MSE) порядка $10^{-5}$ на тестовой выборке.

---

## Преимущества DeepBVP

### 1. Повышение решаемости (Robustness)
Главная ценность проекта не в скорости (простые задачи решаются быстро любым методом), а в **проходимости**. Когда стандартный метод с «линейным стартом» (прямая линия между $\alpha$ и $\beta$) попадает в область расходимости из-за малого $\varepsilon$, нейросеть дает начальную точку, которая уже содержит структуру погранслоя. 

### 2. Масштабируемость сетки
В текущей версии расчет проводится на **500 узлах**, что позволяет детально отрисовывать самые резкие изменения функции (погранслои), предсказанные моделью `V5 Turbo`.

---
## Результаты тестов (5000 испытаний)

Для объективной оценки эффективности был проведен масштабный тест на **5000 случайно сгенерированных задачах**. Сравнение проводилось между стандартным подходом SciPy (линейное начальное приближение) и гибридным подходом **DeepBVP**.

| Показатель | Стандартный метод (Zero Guess) | Гибридный метод (DeepBVP) |
| :--- | :---: | :---: |
| **Общая решаемость (Success Rate)** | 59.0% | **61.3%** |
| **Среднее число вызовов f(x) (nfev)** | 106.3 | **102.3** |
| **Среднее время решения (ms)** | **8.74** | 8.79 |
| **Экономия вычислений** | — | **3.8%** |

### Анализ эффективности



1. **Расширение области сходимости**: 
   Главный результат теста — **139 "спасенных" задач**. Это кейсы, где стандартный алгоритм не смог найти решение (расходимость), а нейросетевое приближение вывело солвер в область притяжения истинного корня. 
   
2. **Надежность**: 
   Случаев, где нейросетевой старт оказался хуже линейного (Linear победил DL), зафиксировано всего **24**. Это дает соотношение **~6:1** в пользу эффективности ИИ-подсказа.

3. **Баланс ресурсов**: 
   Незначительное увеличение среднего времени (на 0.05 ms) связано с накладными расходами на инференс нейросети (PyTorch) и построение сплайнов. Однако это полностью компенсируется снижением количества итераций (`nfev`) и повышением стабильности на жестких уравнениях.

> **Вывод**: Метод DeepBVP не просто ускоряет расчеты, а делает процесс решения более **робастным**, позволяя находить выход там, где классические методы с тривиальным стартом бессильны.

---
## Кейсы "Победы ИИ"

### Кейс №1: Жесткий погранслой
При $\varepsilon = 0.0002$ стандартный решатель часто выдает ошибку `Singular Jacobian`. Гибридный метод использует предсказанный скачок сплайна и успешно сходится.
<img width="1401" height="594" alt="image" src="https://github.com/user-attachments/assets/a2ef7bf6-04cf-4969-a44c-4aede240dc62" />




### Кейс №2: Нелинейные "ловушки"
При сильных нелинейных членах ($j, k$) ландшафт задачи становится многоэкстремальным. Нейросеть, обученная на 27к примерах, сразу направляет решатель в нужную фазу решения, предотвращая расходимость итерационного процесса.
<img width="1366" height="558" alt="image" src="https://github.com/user-attachments/assets/c24f961b-1377-44fa-abc4-98d8132badab" />

---

## Использовать онлайн
Вы можете протестировать решатель в реальном времени через веб-интерфейс:
[**DeepBVP Solver Web App**](https://mlbvpsolver.streamlit.app/)

---

## Векторы развития проекта (планы на будущее)

Проект DeepBVP закладывает фундамент для создания универсальных интеллектуальных решателей. Текущая версия `V5 Turbo` является масштабируемой:

### 1. Переход к адаптивным базисам
Вместо фиксированных 16 кубических сплайнов планируется внедрение **Wavelet-преобразований** или **адаптивных узлов**, это позволит нейросети динамически определять области погранслоя и увеличивать плотность базисных функций там, где решение имеет наибольший градиент.

### 2. Физико-информированное обучение (PINNs)
Интеграция функции потерь, основанной на невязке самого дифференциального уравнения (Residual Loss):
$$L = L_{data} + \lambda \cdot \| \varepsilon y'' + p y' + q y + \dots - f \|^2$$
Это позволит модели обучаться даже на задачах, где нет аналитического или точного численного решения, соблюдая фундаментальные физические законы.

### 3. Повышение точночти существующего решения 

### 4. Интеграция с LLM в качестве распознавателя типа уравнения (если решать несколько типов уравнений с тем же подходом)

---

## Запуск и развертывание

1. **Окружение**: Python 3.10+, `pip install streamlit torch numpy scipy matplotlib joblib`.
2. **Артефакты**: В корне должны быть файлы `model_nn_v5_turbo.pth` и `scalers_v5.pkl`.
3. **Запуск**:
   ```bash
   streamlit run app.py
